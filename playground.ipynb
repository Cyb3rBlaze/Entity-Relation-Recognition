{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "\n",
    "import re\n",
    "\n",
    "import linecache\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['892738', 'breezin', 'album', 'hastype', 'breezin is the fifteenth studio album by jazzsoul guitarist george benson', 'en']\n"
     ]
    }
   ],
   "source": [
    "def clean_up(file_path):\n",
    "    final_data = []\n",
    "    lengths = []\n",
    "\n",
    "    remove_chars = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            for char in remove_chars:\n",
    "                line = line.replace(char, \"\")\n",
    "            \n",
    "            # string clean up\n",
    "            line = re.sub(r'[^a-zA-Z1-9\\s]', '', line)\n",
    "            line = re.sub(' +', ' ', line)\n",
    "            line = line.lower()\n",
    "\n",
    "            line_data = line.split(\"\\t\")\n",
    "\n",
    "            final_data += [line_data]\n",
    "\n",
    "            # sentence lengths\n",
    "            lengths += [len(line_data[4].split(\" \"))]\n",
    "    \n",
    "    return final_data, lengths\n",
    "\n",
    "\n",
    "data, lengths = clean_up('data/en_corpora_test.txt')\n",
    "\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  10.,  224.,  998., 1234., 1284.,  836.,  486.,  250.,  109.,\n",
       "          31.]),\n",
       " array([ 1. ,  5.9, 10.8, 15.7, 20.6, 25.5, 30.4, 35.3, 40.2, 45.1, 50. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sequence length frequencies\n",
    "plt.hist(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "feature_extractor = pipeline('feature-extraction', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 768)\n"
     ]
    }
   ],
   "source": [
    "test = feature_extractor(\"hello how are you\")\n",
    "\n",
    "print(np.array(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, feature_extractor, hidden_size, num_layers=4):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = feature_extractor\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers)\n",
    "\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input_string, hidden):\n",
    "        embedded = torch.tensor(self.embedding(input_string))\n",
    "\n",
    "        final_output = torch.zeros_like(embedded)\n",
    "        \n",
    "        for i in range(embedded.shape[1]):\n",
    "            output, hidden = self.gru(embedded[:,i,:].view(1, 1, -1), hidden)\n",
    "            final_output[:,i,:] = output\n",
    "        \n",
    "        final_output = self.pool(final_output)\n",
    "        final_output = self.sigmoid(final_output)\n",
    "\n",
    "        return final_output.squeeze(2)\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "feature_extractor = pipeline('feature-extraction', model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "model = SimpleGRU(feature_extractor, 768)\n",
    "start_hidden = model.init_hidden()\n",
    "output = model(\"hello how are you\", start_hidden)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(Dataset):\n",
    "    # overriden methods\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.file_path, \"rbU\") as f:\n",
    "            num_lines = sum(1 for _ in f)\n",
    "        \n",
    "        # don't count first line\n",
    "        return num_lines - 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = (idx + 1).tolist()\n",
    "        else:\n",
    "            idx += 1\n",
    "        \n",
    "        particular_line = linecache.getline(self.file_path, idx+1)\n",
    "        cleaned_sample = self.clean_up(particular_line)\n",
    "\n",
    "        input_sentence = cleaned_sample[4]\n",
    "        entity1 = cleaned_sample[1]\n",
    "        entity2 = cleaned_sample[2]\n",
    "\n",
    "        sentence_arr = tokenizer(input_sentence)[\"input_ids\"]\n",
    "        entity1_arr = tokenizer(entity1)[\"input_ids\"]\n",
    "        entity2_arr = tokenizer(entity2)[\"input_ids\"]\n",
    "\n",
    "        labels = torch.zeros((len(sentence_arr)))\n",
    "\n",
    "        for word in sentence_arr:\n",
    "            for entity in entity1_arr:\n",
    "                if entity == word:\n",
    "                    labels[sentence_arr.index(word)] = 1\n",
    "            for entity in entity2_arr:\n",
    "                if entity == word:\n",
    "                    labels[sentence_arr.index(word)] = 1\n",
    "\n",
    "        return input_sentence, labels\n",
    "    \n",
    "\n",
    "    # helper\n",
    "    def clean_up(self, line):\n",
    "        remove_chars = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "\n",
    "        line = line.strip()\n",
    "\n",
    "        for char in remove_chars:\n",
    "            line = line.replace(char, \"\")\n",
    "        \n",
    "        # string clean up\n",
    "        line = re.sub(r'[^a-zA-Z1-9\\s]', '', line)\n",
    "        line = re.sub(' +', ' ', line)\n",
    "        line = line.lower()\n",
    "\n",
    "        line_data = line.split(\"\\t\")\n",
    "        \n",
    "        return line_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9722/1356011285.py:10: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(self.file_path, \"rbU\") as f:\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "dataset = EntityDataset('data/en_corpora_test.txt')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('songs of christmas is the twentysixth album by irish folk music group the irish rovers',), tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 0.]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9722/1356011285.py:10: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(self.file_path, \"rbU\") as f:\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_loader))\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9722/1356011285.py:10: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(self.file_path, \"rbU\") as f:\n",
      "  0%|          | 1/5461 [00:00<1:09:34,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8364523649215698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 51/5461 [00:29<57:24,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7323015928268433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 101/5461 [01:02<53:21,  1.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7921422123908997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 151/5461 [01:33<1:03:51,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7423803210258484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 201/5461 [02:05<58:48,  1.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7458275556564331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 251/5461 [02:38<58:59,  1.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7506291270256042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 301/5461 [03:08<51:32,  1.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7459189295768738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 351/5461 [03:40<40:37,  2.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7190189957618713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 401/5461 [04:12<57:01,  1.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7162379026412964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 451/5461 [04:41<47:51,  1.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6341124773025513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 501/5461 [05:10<51:20,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5753515362739563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 551/5461 [05:39<43:26,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5719085335731506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 601/5461 [06:08<42:26,  1.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6369690895080566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 651/5461 [06:39<48:08,  1.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5962441563606262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 701/5461 [07:05<47:28,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6119452118873596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 751/5461 [07:34<41:44,  1.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49559181928634644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 801/5461 [08:08<1:10:51,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5164920687675476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 850/5461 [08:39<47:00,  1.64it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m labels \u001b[39m=\u001b[39m sample[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m start_hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39;49m[\u001b[39m0\u001b[39;49m], start_hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_string, hidden):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     embedded \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(input_string))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     final_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(embedded)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(embedded\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:107\u001b[0m, in \u001b[0;36mFeatureExtractionPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     98\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    Extract the features of the input(s).\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m        A nested list of `float`: The features computed by the model.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pipelines/base.py:1119\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1112\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1113\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         )\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pipelines/base.py:1126\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1125\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1126\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1127\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1128\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pipelines/base.py:1025\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1024\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1025\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1026\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1027\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:85\u001b[0m, in \u001b[0;36mFeatureExtractionPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m---> 85\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:356\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    354\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m    355\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[0;32m--> 356\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_proj(hidden_states)\n\u001b[1;32m    357\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    358\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pytorch_utils.py:102\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    101\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 102\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    103\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for _, sample in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = sample[0]\n",
    "        labels = sample[1]\n",
    "\n",
    "        start_hidden = model.init_hidden()\n",
    "        output = model(input[0], start_hidden)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if _ % 50 == 0:\n",
    "            print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
