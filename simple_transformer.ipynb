{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced karpathy/minGPT when necessary for transformer implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import linecache\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dk = emb_dim//self.num_heads\n",
    "\n",
    "        # learn projections in one linear operation\n",
    "        self.qkv_proj = nn.Linear(emb_dim, emb_dim*3)\n",
    "        self.out_proj = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.tensor(self.dk, dtype=torch.float32))\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        self.residual_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, emb_dim = x.shape\n",
    "\n",
    "        qkv_combined = self.qkv_proj(x)\n",
    "\n",
    "        # batch size, seq length, num heads, head emb dim\n",
    "        q, k, v = torch.split(qkv_combined, emb_dim, dim=2)\n",
    "\n",
    "        # batch size, num heads, seq length, head emb dim\n",
    "        q = q.view(batch_size, seq_length, self.num_heads, self.dk).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_length, self.num_heads, self.dk).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_length, self.num_heads, self.dk).transpose(1, 2)\n",
    "\n",
    "        # create attention pattern - batch size, num heads, head emb dim, head emb dim\n",
    "        attention_pattern = q @ k.transpose(-2, -1)\n",
    "        attention_pattern = self.softmax(attention_pattern/self.scale)\n",
    "\n",
    "        attention_pattern = self.attention_dropout(attention_pattern)\n",
    "\n",
    "        out = (attention_pattern @ v).transpose(1, 2).contiguous().view(batch_size, seq_length, emb_dim)\n",
    "\n",
    "        return self.residual_dropout(self.out_proj(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8611/2606639175.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_pattern = self.softmax(attention_pattern/self.scale)\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dk = emb_dim//self.num_heads\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim) # layer norm over the embedding dimension\n",
    "        self.attention = MultiHeadAttention(emb_dim, num_heads, dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 4), # fully connected\n",
    "            nn.Linear(emb_dim * 4, emb_dim), # projection back into original embedding dims\n",
    "            GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "block = TransformerBlock(50, 2, 0.1)\n",
    "# 4 batch size, 10 seq length, 50 emb dimension\n",
    "sample_input = torch.randn((4, 10, 50))\n",
    "\n",
    "output = block(sample_input)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, dropout, embedding_path, vocab_path, num_layers=4):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        word_embeddings = np.load(embedding_path)\n",
    "        self.vocab = np.load(vocab_path)\n",
    "\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(torch.from_numpy(word_embeddings).float())\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        for i in range(num_layers):\n",
    "            self.transformer_blocks += [TransformerBlock(emb_dim, num_heads, dropout)]\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList(self.transformer_blocks)\n",
    "\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "\n",
    "        final_output = torch.zeros_like(embeddings)\n",
    "        \n",
    "        for layer in self.transformer_blocks:\n",
    "            final_output = layer(final_output)\n",
    "        \n",
    "        final_output = self.pool(final_output)\n",
    "        final_output = self.sigmoid(final_output)\n",
    "\n",
    "        return final_output.squeeze(2)\n",
    "\n",
    "\n",
    "    def get_indices(self, string):\n",
    "        indices = []\n",
    "\n",
    "        string_arr = string.split(\" \")\n",
    "\n",
    "        for word in string_arr:\n",
    "            if np.where(self.vocab == word)[0].shape[0] != 0:\n",
    "                indices += [np.where(self.vocab == word)[0]]\n",
    "            else:\n",
    "                indices += [np.array([0])]\n",
    "        \n",
    "        return torch.tensor(indices).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(Dataset):\n",
    "    # overriden methods\n",
    "    def __init__(self, file_path, model):\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.file_path, \"rbU\") as f:\n",
    "            num_lines = sum(1 for _ in f)\n",
    "        \n",
    "        # don't count first line\n",
    "        return num_lines - 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        particular_line = linecache.getline(self.file_path, idx+1)\n",
    "        cleaned_sample = self.clean_up(particular_line)\n",
    "\n",
    "        input_sentence = cleaned_sample[4]\n",
    "        entity1 = cleaned_sample[1]\n",
    "        entity2 = cleaned_sample[2]\n",
    "\n",
    "        sentence_arr = self.model.get_indices(input_sentence)\n",
    "        entity1_arr = self.model.get_indices(entity1)\n",
    "        entity2_arr = self.model.get_indices(entity2)\n",
    "\n",
    "        '''labels = torch.zeros_like(sentence_arr)\n",
    "\n",
    "        for _, word_index in enumerate(sentence_arr):\n",
    "            for entity_index in entity1_arr:\n",
    "                if entity_index == word_index:\n",
    "                    labels[_] = 1\n",
    "            for entity_index in entity2_arr:\n",
    "                if entity_index == word_index:\n",
    "                    labels[_] = 1'''\n",
    "\n",
    "        labels1 = self.generate_labels(sentence_arr, entity1_arr)\n",
    "        labels2 = self.generate_labels(sentence_arr, entity2_arr)\n",
    "\n",
    "        labels = labels1 | labels2\n",
    "\n",
    "        return sentence_arr, labels.type(torch.float32)\n",
    "    \n",
    "\n",
    "    # first instance of entity in sentence\n",
    "    def generate_labels(self, sentence_arr, entity_arr):\n",
    "        correct = []\n",
    "\n",
    "        for _, token in enumerate(sentence_arr):\n",
    "            if _ < sentence_arr.shape[0] - entity_arr.shape[0]:\n",
    "                if token == entity_arr[0]:\n",
    "                    not_equal = False\n",
    "                    for i, val in enumerate(entity_arr):\n",
    "                        if sentence_arr[_+i] != val:\n",
    "                            not_equal = True\n",
    "                    if not not_equal:\n",
    "                        correct += [_]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        labels = torch.zeros_like(sentence_arr)\n",
    "        for i in correct:\n",
    "            labels[i:i+entity_arr.shape[0]] = 1\n",
    "        \n",
    "        return labels\n",
    "        \n",
    "\n",
    "    # helper\n",
    "    def clean_up(self, line):\n",
    "        remove_chars = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "\n",
    "        line = line.strip()\n",
    "\n",
    "        for char in remove_chars:\n",
    "            line = line.replace(char, \"\")\n",
    "        \n",
    "        # string clean up\n",
    "        line = re.sub(r'[^a-zA-Z1-9\\s]', '', line)\n",
    "        line = re.sub(' +', ' ', line)\n",
    "        line = line.lower()\n",
    "\n",
    "        line_data = line.split(\"\\t\")\n",
    "        \n",
    "        return line_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8611/2606639175.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_pattern = self.softmax(attention_pattern/self.scale)\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello how are you\"\n",
    "\n",
    "model = SimpleTransformer(50, 5, 0.05, \"utils/embs_npa.npy\", \"utils/vocab_npa.npy\")\n",
    "\n",
    "indices = model.get_indices(test_string)\n",
    "\n",
    "print(indices.shape)\n",
    "\n",
    "output = model(indices.unsqueeze(0))\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8611/1136066156.py:10: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(self.file_path, \"rbU\") as f:\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "dataset = EntityDataset('data/en_corpora_test.txt', model)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8611/1136066156.py:10: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(self.file_path, \"rbU\") as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   518,   1527,     16,      9,   2843,    284,      8,     89,   2539,\n",
      "            984,     23,  12072,  13037,   1016,     23,    281,   2412,      7,\n",
      "           1683,     23,   3022, 150915]]), tensor([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_loader))\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8611/1136066156.py:10: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(self.file_path, \"rbU\") as f:\n",
      "  0%|          | 0/5461 [00:00<?, ?it/s]/tmp/ipykernel_8611/2606639175.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_pattern = self.softmax(attention_pattern/self.scale)\n",
      "  0%|          | 1/5461 [00:00<30:42,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8730854392051697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 51/5461 [00:13<20:23,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030960917472839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 102/5461 [00:28<25:25,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6933606863021851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 151/5461 [00:42<23:25,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6939178109169006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 201/5461 [00:56<23:05,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6913807988166809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 251/5461 [01:09<21:27,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 301/5461 [01:23<24:26,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 352/5461 [01:37<19:22,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6915850043296814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 401/5461 [01:50<23:34,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6892816424369812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 451/5461 [02:05<32:26,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6933079957962036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 501/5461 [02:19<23:33,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6889674067497253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 552/5461 [02:34<16:49,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6892901062965393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 601/5461 [02:49<23:31,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6829128265380859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 651/5461 [03:05<25:01,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6906524300575256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 702/5461 [03:19<22:56,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471228599548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 751/5461 [03:34<21:59,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934482455253601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 801/5461 [03:48<23:12,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6940341591835022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 851/5461 [04:03<22:53,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6900690793991089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 901/5461 [04:17<22:31,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6838020086288452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 952/5461 [04:31<24:42,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6932098865509033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1001/5461 [04:45<21:47,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6771064400672913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1005/5461 [04:47<21:13,  3.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb Cell 12\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _, sample \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m sample[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb Cell 12\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m entity1 \u001b[39m=\u001b[39m cleaned_sample[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m entity2 \u001b[39m=\u001b[39m cleaned_sample[\u001b[39m2\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m sentence_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mget_indices(input_sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m entity1_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_indices(entity1)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m entity2_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_indices(entity2)\n",
      "\u001b[1;32m/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb Cell 12\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m string_arr \u001b[39m=\u001b[39m string\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m string_arr:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab \u001b[39m==\u001b[39;49m word)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m         indices \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m==\u001b[39m word)[\u001b[39m0\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/entity-relation-recognition/simple_transformer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for _, sample in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = sample[0]\n",
    "        labels = sample[1]\n",
    "\n",
    "        output = model(input)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if _ % 50 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000, 0.5113, 0.5000, 0.5027, 0.5000]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8611/2606639175.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_pattern = self.softmax(attention_pattern/self.scale)\n"
     ]
    }
   ],
   "source": [
    "test_string = \"I am a very cool person\"\n",
    "\n",
    "indices = model.get_indices(test_string)\n",
    "\n",
    "output = model(indices.unsqueeze(0))\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
